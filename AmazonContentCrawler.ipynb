{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "USER_AGENT = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "\n",
    "## GET THE HTML DOM ELEMENT\n",
    "def fetch_results(node_id):\n",
    "    amazon_help_url = 'https://www.amazon.com/gp/help/customer/display.html?nodeId={}&pop-up=1'.format(node_id)\n",
    "    response = requests.get(amazon_help_url, headers=USER_AGENT)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def extractTable(table):\n",
    "    data = ''\n",
    "    colnames = []\n",
    "    for children in table:\n",
    "        if children.name == \"caption\":           \n",
    "            data = data + ' ' + children.text.strip() + ': '\n",
    "        if children.name == \"thead\":\n",
    "            cols = children.find_all('th')\n",
    "            for c in cols:\n",
    "                colnames.append(c.text.strip())\n",
    "        if children.name == \"tbody\":\n",
    "            rows = children.find_all('tr')\n",
    "            for r in rows:\n",
    "                cols = r.find_all('td')\n",
    "                count = 0\n",
    "                sentinel = ''\n",
    "                if len(colnames) == len(cols):\n",
    "                    for c in cols:\n",
    "                        data = data + sentinel + colnames[count] + '=' + getData(c).strip()\n",
    "                        sentinel = ', '\n",
    "                        count += 1\n",
    "                    data = data + '. '\n",
    "                else:\n",
    "                    for c in cols:\n",
    "                        data = data + sentinel +getData(c).strip()\n",
    "                        sentinel = ', '\n",
    "                        count += 1\n",
    "                    data = data + '. '\n",
    "                \n",
    "                \n",
    "    return data\n",
    "\n",
    "def getData(node, ignore_tags=(\"script\", \"style\")):\n",
    "    if node.name is None:\n",
    "        return node\n",
    "    text = ''\n",
    "    for children in node:\n",
    "        if children.name in ignore_tags:\n",
    "            continue;\n",
    "        elif children.name == \"h1\" or children.name == \"h2\" or children.name == \"h3\":\n",
    "            temp = children.text\n",
    "            if len(temp) > 0 and temp[len(temp) - 1] in (':', '?', '.'):\n",
    "                text = text.rstrip() + ' ' + temp[:-1].rstrip() + ': '\n",
    "            else:\n",
    "                text = text.rstrip() + ' ' + temp.rstrip() + ': '\n",
    "        elif children.name == \"a\" and \"href\" in children.attrs and len(children[\"href\"]) > 0:\n",
    "            text = text.rstrip() + ' [' + children.text.strip() + '](' + ('www.amazon.com' + children[\"href\"] if children[\"href\"][0] == \"/\" else children[\"href\"]) + ')'\n",
    "        elif children.name == \"table\":\n",
    "            text = text.rstrip() + extractTable(children)\n",
    "        else:\n",
    "            temp = getData(children)\n",
    "            if len(temp) > 1 and temp[0:2] == \" [\":\n",
    "                text = text.rstrip() + temp\n",
    "            else:\n",
    "                text = text + temp\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def get_text(node_id):\n",
    "    html = fetch_results(node_id)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.text.replace('Amazon.com Help: ','').strip()\n",
    "    soup = soup.find_all('div', attrs={'class': ['help-content','cs-help-content']})[0]\n",
    "    a = re.sub(\"\\s*[\\.]+\\s*[\\n]+\\s*\",\". \",getData(soup).strip())\n",
    "    a = re.sub(\"[:]+\\s*[\\n]+\\s*\",\": \",a)\n",
    "    a = re.sub(\"[\\?]+\\s*[\\n]+\\s*\",\"? \",a)\n",
    "    a = re.sub(r\"(\\s*[\\n]+\\s*)([A-Z])\",r\". \\2\",a)\n",
    "    a = re.sub(\"\\s*[\\n]+\\s*\",\" \",a)\n",
    "    return title,a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & Debugging Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_text('GMDKZSFRSX7NKKN3')\n",
    "html = fetch_results('201937120')\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "title = soup.title.text.replace('Amazon.com Help: ','').strip()\n",
    "soup = soup.find_all('div', attrs={'class': ['help-content','cs-help-content']})[0]\n",
    "a = re.sub(\"\\s*[\\.]+\\s*[\\n]+\\s*\",\". \",getData(soup).strip())\n",
    "a = re.sub(\"[:]+\\s*[\\n]+\\s*\",\": \",a)\n",
    "a = re.sub(\"[\\?]+\\s*[\\n]+\\s*\",\"? \",a)\n",
    "a = re.sub(r\"(\\s*[\\n]+\\s*)([A-Z])\",r\". \\2\",a)\n",
    "a = re.sub(\"\\s*[\\n]+\\s*\",\" \",a)\n",
    "print(title,a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break Answer for Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blurb:\n",
    "    def __init__(self,type,value,relativeUrl,desktopRelativeUrl):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "        self.relativeUrl = relativeUrl\n",
    "        self.desktopRelativeUrl = desktopRelativeUrl\n",
    "        self.buttons = []\n",
    "        \n",
    "    def addButton(self,x):\n",
    "        self.buttons.append(vars(x))\n",
    "        \n",
    "class Button:\n",
    "    def __init__(self,text,type,value):\n",
    "        self.text = text\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "\n",
    "### devide by peroid and merge\n",
    "def split_url_ans(text, pattern = r'\\]\\([^(]+\\)'):\n",
    "#     pattern = r'\\((?:https:\\/\\/)?www.[^(]+\\)'\n",
    "    url_list = [a[2:][:-1] for a in re.findall(pattern, text)]\n",
    "    ans_list = []\n",
    "    ans_raw = re.split(pattern, text)\n",
    "    for i, a in enumerate(ans_raw):\n",
    "        if i == len(ans_raw) - 1:\n",
    "            ans_list.append(a.strip(','))\n",
    "        else: \n",
    "            ans_list.append((a + ']').strip(',').strip())\n",
    "    return ans_list, url_list\n",
    "\n",
    "\n",
    "def get_answer_list(answers):\n",
    "    alist = answers.strip().split('. ')\n",
    "    out_alist=[]\n",
    "    for a in alist:\n",
    "        answer = a.strip()\n",
    "        ans_list, url_list = split_url_ans(answer)\n",
    "        if len(url_list) > 0:\n",
    "            for i, u in enumerate(url_list):\n",
    "                a = re.sub(r'\\[(.*)\\]', r'<a>\\1</a>', ans_list[i])\n",
    "                if i == len(url_list)-1:\n",
    "                    ans = (a+ans_list[i+1]).strip()\n",
    "                    highlights = re.sub(r'\\<a\\>(.*)\\<\\/a\\>', r'\\1', ans)\n",
    "                    if len(highlights) > 0:\n",
    "                        out_alist.append([ans,u])\n",
    "                else: \n",
    "                    out_alist.append([a.strip(),u])\n",
    "        else: \n",
    "            out_alist.append([answer,''])\n",
    "    return out_alist\n",
    "\n",
    "\n",
    "def reformat_answer(text,max_ans_len = 300):\n",
    "    answer_list = get_answer_list(text)\n",
    "    i=0\n",
    "    j=1\n",
    "    ans = answer_list[0][0]\n",
    "    url = answer_list[0][1]\n",
    "    while j < len(answer_list):\n",
    "        next_ans = answer_list[j][0]\n",
    "        next_url = answer_list[j][1]\n",
    "        url_check = 1 if len(url) > 0 and len(next_url) > 0 else 0\n",
    "        if len(ans) + len(next_ans) < max_ans_len and url_check == 0:\n",
    "            ans += '. ' + next_ans\n",
    "            url += next_url\n",
    "            if j == len(answer_list)-1:\n",
    "                answer_list[i] = [ans,url]\n",
    "        else:\n",
    "            answer_list[i] = [ans,url]\n",
    "            ans = answer_list[j][0]\n",
    "            url = answer_list[j][1]\n",
    "            if j == len(answer_list)-1:\n",
    "                answer_list[i+1] = [ans,url]\n",
    "            i += 1\n",
    "        j+=1\n",
    "\n",
    "    formatted_answer = answer_list[:i+1]\n",
    "    final_answer = [vars(Blurb('ANSWER',blurb[0],blurb[1],blurb[1])) for blurb in formatted_answer]\n",
    "    return final_answer\n",
    "\n",
    "reformat_answer('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Top Help Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#top_160_documents = pd.read_csv('/data/qyouran/QABot/QAData/HelpDoc_InitialLaunch/doc_coverage/df_help_document_2_top160.tsv',sep='\\t')\n",
    "#top_queries = pd.read_csv('/data/qyouran/QABot/QAData/HelpDoc_InitialLaunch/doc_coverage/prediction_2_top160.tsv',sep='\\t')\n",
    "top_200_answers = pd.read_csv('/data/qyouran/QABot/QAData/HelpDoc_InitialLaunch/doc_coverage/df_help_document_2_answer_top200.tsv',sep='\\t')\n",
    "top_answers = pd.read_csv('/data/qyouran/QABot/QAData/HelpDoc_InitialLaunch/doc_coverage/prediction_2_answer_top200.tsv',sep='\\t')\n",
    "with open('node_id_added_from_top160', 'rb') as f:\n",
    "    node_id_added_from_top160 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = top_answers.head(10)['query'].to_list()\n",
    "# ## s3 initialize\n",
    "# os.environ['AWS_ACCESS_KEY_ID']='<ACCESS KEY>'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']='<SECRET KEY>'\n",
    "# BUCKET_NAME_INPUT='qabot-annotation-input-ir-b'\n",
    "# s3_client = boto3.client('s3')\n",
    "\n",
    "# for i,query in enumerate(queries):\n",
    "#     filename=str(i+1)\n",
    "#     body=query\n",
    "#     s3_client.put_object(Body=body,Bucket=BUCKET_NAME_INPUT,Key=filename+'.txt',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "output = dict()\n",
    "offset = 1\n",
    "new_list = ['W3L8JX7Q9FH8ALB', 'TZVXNQGEL3K9CSH']\n",
    "nodes = node_id_added_from_top160 + top_200_answers.node_id.tolist() + new_list\n",
    "\n",
    "\n",
    "for i,node_id in enumerate(nodes):\n",
    "    try:\n",
    "        nid = 'G'+str(node_id)\n",
    "        url = 'https://www.amazon.com/gp/help/customer/display.html?nodeId={}&pop-up=1'.format(nid)\n",
    "        pid = offset + i\n",
    "        title, passage = get_text(nid)\n",
    "        output[pid] = [passage, nid, url, title]\n",
    "        \n",
    "    except:\n",
    "        print('retrying for pid: {}, node_id: {}'.format(offset + i, node_id))\n",
    "        sleep(randint(2,4)) \n",
    "        nid = 'G'+str(node_id)\n",
    "        url = 'https://www.amazon.com/gp/help/customer/display.html?nodeId={}&pop-up=1'.format(nid)\n",
    "        pid = offset + i\n",
    "        title, passage = get_text(nid)\n",
    "        output[pid] = [passage, nid, url, title]\n",
    "\n",
    "with open(\"/data/QAData/InformationRetrievalData/amazon/helpdocuments_collection_new.json\", \"w\") as outfile:  \n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Production Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "sys.path.append('/home/srikamma/efs/workspace/CS-QASystem-Torch/src/CS-QASystem-Torch/src/cs_qa_system_torch/')\n",
    "from factory.word_tokenizer_factory import english_preprocessor\n",
    "\n",
    "def get_preprocessed_document_ir(text):\n",
    "    english_preprocessor.do_apostr_contract=True\n",
    "    english_preprocessor.do_apostr_possess = True\n",
    "    english_preprocessor.noise_regex['URL'] = (re.compile(r'(\\(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^)\\s]{2,}|www\\.[a-zA-Z0-9]' \\\n",
    "                                                      r'[a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^)\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^)\\s]{2,}|www\\.' \\\n",
    "                                                      r'[a-zA-Z0-9]+\\.[^)\\s]{2,})'), )\n",
    "    passage_ir = english_preprocessor.preprocess_single(text)\n",
    "    return passage_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "collection = json.load(open('/data/QAData/InformationRetrievalData/amazon/helpdocuments_collection_new.json','r'))\n",
    "outfile = '/data/QAData/InformationRetrievalData/amazon/production_collection.json'\n",
    "outfile_url = '/data/QAData/InformationRetrievalData/amazon/production_collection_url.json'\n",
    "# prod_pid_excluded = [1, 4, 5, 12, 13, 31, 35, 75, 98, 103, 112, 149, 183, 193] #122, 38 removed from this list\n",
    "# digital_pid = [2, 9, 10, 11, 22, 30, 32, 36, 38, 44, 46, 47, 65, 69, 80, 91, 95, 96, 129, 131, 137, 138, 148, 150, 151, 155, 173, 178, 184, 185, 189, 191, 196, 199, 225, 237, 243]\n",
    "ignored_node_ids = ['G4KFSMKGWZPYNW9X', 'G202185710', 'G8P3YMHU34R3XX4N', 'G202120810', 'G201911080', 'G201910410', 'GZ3PDPNA7U6R6UMK', \n",
    "                    'GAEJPJ8E5TY8TTNL', 'GWGDSNXVPJ93UW5V', 'GSD587LKW72HKU2V', 'G3BQ95AL4WZLV9ZG', 'G200127470', 'GL4263XHAGWBSC8R', \n",
    "                    'G64ENL4SCTZ4EXSY']\n",
    "digital_node_ids = ['GJT6X5TZUW8AB9Y9', 'G201567520', 'G9JFV7VRANZDKKWG', 'GW5P2J5UV6EHRZCL', 'GE8EWPM8E5QG8E8E', 'G201555990', 'G201890100', \n",
    "                    'GXQJG7FBB6SJD22T', 'G3CHA35W7N58VG5B', 'GTQEND3RFAFNLKU5', 'G201609150', 'GSNFRLBMD26UK9PA', 'GQ8QNEVL5FCKBGH2', \n",
    "                    'G8637923FFWAR2YH', 'GZCHXL8CUW3VWJQP', 'G202070170', 'GT4SYLY6SVG9QWBV', 'G937D322PWZ6L9BL', 'GEA2QYCTZQXKRG4Z', \n",
    "                    'G3EWVFQZCVKH53TB', 'G201755180', 'G202020180', 'GDMMKH7RJP2HU2P2', 'GLSQ4722655M4ZEJ', 'GCRZL3F2UZMNP3T3', \n",
    "                    'GRTHB5PBJ32UUSMT', 'G202161160', 'GHANKAZLVY3SKWMV', 'G7CF8KV3YPXE285Y', 'GQ2FT94RCVV5BA3Z', 'GWWLWFYVTYQTLLL9', \n",
    "                    'G3AJT9URG45M44HB', 'GAZ2TKL8VEEQUVRC', 'G202200040', 'GLSQSWPWZMLR3RA5', 'G201820380', 'G8XJWJRQCYN3PG6L']\n",
    "\n",
    "output = dict()\n",
    "output_url = dict()\n",
    "length_threshold = 420\n",
    "\n",
    "for key,val in collection.items():\n",
    "    if val[1] in ignored_node_ids+digital_node_ids or len(val[0].split()) > length_threshold:\n",
    "        print('Dropping: {}'.format(val[2]))\n",
    "        continue\n",
    "    output[key] = [val[0],get_preprocessed_document_ir(val[0])]\n",
    "    output_url[key] = [val[2],val[3]]\n",
    "\n",
    "with open(outfile, \"w\") as f:  \n",
    "    json.dump(output, f)\n",
    "    \n",
    "with open(outfile_url, \"w\") as f:  \n",
    "    json.dump(output_url, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "collection = json.load(open('/data/QAData/InformationRetrievalData/amazon/helpdocuments_collection_new.json','r'))\n",
    "outfile = '/data/QAData/InformationRetrievalData/amazon/production_collection.json'\n",
    "outfile_url = '/data/QAData/InformationRetrievalData/amazon/production_collection_url.json'\n",
    "#prod_pid_excluded = [1, 4, 5, 12, 13, 31, 35, 75, 98, 103, 112, 149, 183, 193] #122, 38 removed from this list\n",
    "# digital_pid = [2, 9, 10, 11, 22, 30, 32, 36, 38, 44, 46, 47, 65, 69, 80, 91, 95, 96, 129, 131, 137, 138, 148, 150, 151, 155, 173, 178, 184, 185, 189, 191, 196, 199, 225, 237, 243]\n",
    "ignored_node_ids = ['G4KFSMKGWZPYNW9X', 'G202185710', 'G8P3YMHU34R3XX4N', 'G202120810', 'G201911080', 'G201910410', 'GZ3PDPNA7U6R6UMK', \n",
    "                    'GAEJPJ8E5TY8TTNL', 'GWGDSNXVPJ93UW5V', 'GSD587LKW72HKU2V', 'G3BQ95AL4WZLV9ZG', 'G200127470', 'GL4263XHAGWBSC8R', \n",
    "                    'G64ENL4SCTZ4EXSY']\n",
    "digital_node_ids = ['GJT6X5TZUW8AB9Y9', 'G201567520', 'G9JFV7VRANZDKKWG', 'GW5P2J5UV6EHRZCL', 'GE8EWPM8E5QG8E8E', 'G201555990', 'G201890100', \n",
    "                    'GXQJG7FBB6SJD22T', 'G3CHA35W7N58VG5B', 'GTQEND3RFAFNLKU5', 'G201609150', 'GSNFRLBMD26UK9PA', 'GQ8QNEVL5FCKBGH2', \n",
    "                    'G8637923FFWAR2YH', 'GZCHXL8CUW3VWJQP', 'G202070170', 'GT4SYLY6SVG9QWBV', 'G937D322PWZ6L9BL', 'GEA2QYCTZQXKRG4Z', \n",
    "                    'G3EWVFQZCVKH53TB', 'G201755180', 'G202020180', 'GDMMKH7RJP2HU2P2', 'GLSQ4722655M4ZEJ', 'GCRZL3F2UZMNP3T3', \n",
    "                    'GRTHB5PBJ32UUSMT', 'G202161160', 'GHANKAZLVY3SKWMV', 'G7CF8KV3YPXE285Y', 'GQ2FT94RCVV5BA3Z', 'GWWLWFYVTYQTLLL9', \n",
    "                    'G3AJT9URG45M44HB', 'GAZ2TKL8VEEQUVRC', 'G202200040', 'GLSQSWPWZMLR3RA5', 'G201820380', 'G8XJWJRQCYN3PG6L']\n",
    "\n",
    "output = dict()\n",
    "output_url = dict()\n",
    "length_threshold = 420\n",
    "docs = []\n",
    "\n",
    "for key,val in collection.items():\n",
    "    if val[1] in ignored_node_ids or val[1] in digital_node_ids or len(val[0].split()) > length_threshold:\n",
    "        print('Dropping: {}'.format(val[2]))\n",
    "        continue\n",
    "    output[key] = [val[0],get_preprocessed_document_ir(val[0])]\n",
    "    output_url[key] = val[2]\n",
    "    doc = dict()\n",
    "    doc['pid'] = key\n",
    "    doc['title'] = val[3]\n",
    "    doc['url'] = val[2]\n",
    "    doc['passage'] = val[0]\n",
    "    docs.append(doc)\n",
    "documents = pd.DataFrame(docs)\n",
    "documents.to_csv('production_documents.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Length Distribution of collection\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "output = json.load(open('/data/QAData/InformationRetrievalData/amazon/production_collection.json','r'))\n",
    "x = []\n",
    "for key, val in output.items():\n",
    "    x.append(len(val[0].split()))\n",
    "plt.hist(x)\n",
    "z = np.array(x)\n",
    "np.quantile(z,0.80)\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Production Document Collection for Dialing Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_document = pd.read_excel('/data/QAData/InformationRetrievalData/amazon/final_document_T2_dialup_completed_with_performance_confirmed.xlsx').fillna('')\n",
    "\n",
    "print(df_document['should_exclude'].value_counts())\n",
    "\n",
    "# Explanations to the labels in \"should_exclude\" column:\n",
    "# 4: duplicated or empty documents\n",
    "# 3: link farms\n",
    "# 2: documents with poor performance identified in MLDA's deep dive\n",
    "# 1: documents that seem to be relevant to very few queries but could mislead the model (model could incorrectly rank them higher than the true relevant documents)\n",
    "# 0: all the other documents we will keep (documents with pid 77, 127, 145, 227 seem to be D2 related, but we keep them for now)\n",
    "df_document_selected = df_document.loc[df_document['should_exclude'] == 0, :]\n",
    "\n",
    "df_document_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert not (df_document_selected['pid'].isnull()).any()\n",
    "assert not (df_document_selected['new_url'] == '').any()\n",
    "assert not (df_document_selected['new_title'] == '').any()\n",
    "assert not (df_document_selected['new_passage'] == '').any()\n",
    "assert not (df_document_selected['new_passage_preprocessed'] == '').any()\n",
    "\n",
    "assert not df_document_selected['pid'].duplicated().any()\n",
    "assert not df_document_selected['new_url'].duplicated().any()\n",
    "assert not df_document_selected['new_title'].duplicated().any()\n",
    "assert not df_document_selected['new_passage'].duplicated().any()\n",
    "assert not df_document_selected['new_passage_preprocessed'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict()\n",
    "output_url = dict()\n",
    "\n",
    "for i in df_document_selected.index:\n",
    "    pid = df_document_selected.loc[i, 'pid']\n",
    "    url = df_document_selected.loc[i, 'new_url']\n",
    "    title = df_document_selected.loc[i, 'new_title']\n",
    "    passage = df_document_selected.loc[i, 'new_passage']\n",
    "    passage_preprocessed = df_document_selected.loc[i, 'new_passage_preprocessed']\n",
    "    \n",
    "    output[str(pid)] = [passage, passage_preprocessed]\n",
    "    output_url[str(pid)] = [url, title]\n",
    "\n",
    "with open('/data/QAData/InformationRetrievalData/amazon/production_collection.json', 'w') as f:  \n",
    "    json.dump(output, f)\n",
    "\n",
    "with open('/data/QAData/InformationRetrievalData/amazon/production_collection_url.json', 'w') as f:  \n",
    "    json.dump(output_url, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating AE Annotation into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "collection = json.load(open('/data/QAData/InformationRetrievalData/amazon/helpdocuments_collection_new.json','r'))\n",
    "outfile = '/data/QAData/InformationRetrievalData/amazon/production_collection.json'\n",
    "#prod_pid_excluded = [1, 4, 5, 12, 13, 31, 35, 75, 98, 103, 112, 149, 183, 193] #122, 38 removed from this list\n",
    "# digital_pid = [2, 9, 10, 11, 22, 30, 32, 36, 38, 44, 46, 47, 65, 69, 80, 91, 95, 96, 129, 131, 137, 138, 148, 150, 151, 155, 173, 178, 184, 185, 189, 191, 196, 199, 225, 237, 243]\n",
    "ignored_node_ids = ['G4KFSMKGWZPYNW9X', 'G202185710', 'G8P3YMHU34R3XX4N', 'G202120810', 'G201911080', 'G201910410', 'GZ3PDPNA7U6R6UMK', \n",
    "                    'GAEJPJ8E5TY8TTNL', 'GWGDSNXVPJ93UW5V', 'GSD587LKW72HKU2V', 'G3BQ95AL4WZLV9ZG', 'G200127470', 'GL4263XHAGWBSC8R', \n",
    "                    'G64ENL4SCTZ4EXSY']\n",
    "digital_node_ids = ['GJT6X5TZUW8AB9Y9', 'G201567520', 'G9JFV7VRANZDKKWG', 'GW5P2J5UV6EHRZCL', 'GE8EWPM8E5QG8E8E', 'G201555990', 'G201890100', \n",
    "                    'GXQJG7FBB6SJD22T', 'G3CHA35W7N58VG5B', 'GTQEND3RFAFNLKU5', 'G201609150', 'GSNFRLBMD26UK9PA', 'GQ8QNEVL5FCKBGH2', \n",
    "                    'G8637923FFWAR2YH', 'GZCHXL8CUW3VWJQP', 'G202070170', 'GT4SYLY6SVG9QWBV', 'G937D322PWZ6L9BL', 'GEA2QYCTZQXKRG4Z', \n",
    "                    'G3EWVFQZCVKH53TB', 'G201755180', 'G202020180', 'GDMMKH7RJP2HU2P2', 'GLSQ4722655M4ZEJ', 'GCRZL3F2UZMNP3T3', \n",
    "                    'GRTHB5PBJ32UUSMT', 'G202161160', 'GHANKAZLVY3SKWMV', 'G7CF8KV3YPXE285Y', 'GQ2FT94RCVV5BA3Z', 'GWWLWFYVTYQTLLL9', \n",
    "                    'G3AJT9URG45M44HB', 'GAZ2TKL8VEEQUVRC', 'G202200040', 'GLSQSWPWZMLR3RA5', 'G201820380', 'G8XJWJRQCYN3PG6L']\n",
    "\n",
    "output = dict()\n",
    "length_threshold = 420\n",
    "\n",
    "\n",
    "for key,val in collection.items():\n",
    "    if val[1] in ignored_node_ids or val[1] in digital_node_ids or len(val[0].split()) > length_threshold:\n",
    "        print('Dropping: {}'.format(val[2]))\n",
    "        continue\n",
    "    output[key] = val[2] + '\\t' + val[0]\n",
    "\n",
    "with open(\"annotation_collection.json\", \"w\") as outfile:  \n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from json import JSONDecodeError\n",
    "\n",
    "## load to s3 for annotation\n",
    "\n",
    "annotation_file = 'annotation_collection.json'\n",
    "try:\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        collection = json.load(f)\n",
    "except (JSONDecodeError, FileNotFoundError) as error:\n",
    "    print('Error: please make sure the path is correct and the file is a json file')\n",
    "    raise error\n",
    "\n",
    "\n",
    "## s3 initialize\n",
    "os.environ['AWS_ACCESS_KEY_ID']='<replace with access key CS-ML-ANALYTICS account>'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='replace with secret key of CS-ML-ANALYTICS account'\n",
    "BUCKET_NAME_INPUT='qabot-annotation-input-ae'\n",
    "BUCKET_NAME_OUTPUT='qabot-annotation-output-ae'\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for k,v in collection.items():\n",
    "    filename=str(k)\n",
    "    body=v\n",
    "    s3_client.put_object(Body=body,Bucket=BUCKET_NAME_INPUT,Key=filename+'.txt',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Annotated Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "pd.set_option('colwidth',None)\n",
    "def remove_urls(df):\n",
    "    my_url_regex = r\"(\\(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^)\\s]{2,}|www\\.[a-zA-Z0-9]\" \\\n",
    "                   r\"[a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^)\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^)\\s]{2,}|www\\.\" \\\n",
    "                   r\"[a-zA-Z0-9]+\\.[^)\\s]{2,})\"\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"----- Will remove all urls in passage and answer and replace them with (URL) -----\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index, \"passage\"] = re.sub(my_url_regex, \"URL\", row[\"passage\"])\n",
    "        df.at[index, \"answer\"] = re.sub(my_url_regex, \"URL\", row[\"answer\"])\n",
    "    return df\n",
    "\n",
    "def process_annotation_data(out_file_path:str):\n",
    "    ## s3 initialize\n",
    "    os.environ['AWS_ACCESS_KEY_ID']='<ACCESS KEY>'\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY']='SECRET KEY'\n",
    "    BUCKET_NAME_INPUT='qabot-annotation-input-ae'\n",
    "    BUCKET_NAME_OUTPUT='qabot-annotation-output-ae'\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    try:\n",
    "        s3_obj = s3_client.get_object(Bucket=BUCKET_NAME_OUTPUT,Key='count.txt')\n",
    "        s3_data = s3_obj['Body'].read()\n",
    "        count = int(s3_data)\n",
    "    except:\n",
    "        count = 0 \n",
    "\n",
    "    out = []\n",
    "    for key in range(1,count+1):\n",
    "        try:\n",
    "            s3_obj = s3_client.get_object(Bucket=BUCKET_NAME_OUTPUT,Key=str(key)+'.txt')\n",
    "            s3_data = s3_obj['Body']._raw_stream.readline().decode('utf-8')\n",
    "            data = json.loads(s3_data)\n",
    "            temp = dict()\n",
    "            temp['qid'] = key\n",
    "            temp['query'] = data['question']\n",
    "            temp['answer'] = data['answer']\n",
    "            temp['passage'] = data['content']\n",
    "            out.append(temp)\n",
    "        except:\n",
    "            continue\n",
    "    out_df = pd.DataFrame(out)\n",
    "    remove_urls(out_df)\n",
    "    out_df.to_csv(out_file_path,sep='\\t',index=None)\n",
    "    return out_df\n",
    "\n",
    "result = process_annotation_data('/data/QAData/AnswerExtractionData/amazon/train_finetune.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
