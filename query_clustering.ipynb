{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import clear_output\n",
    "from sklearn.manifold import MDS  # The version of \"sklearn\" I used is 0.22.2.post1\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS, KMeans, AgglomerativeClustering, DBSCAN\n",
    "from time import sleep\n",
    "from torch import save, load\n",
    "from sentence_transformers import SentenceTransformer  # The version of \"sentence-transformers\" I used is 0.3.2\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.linalg import norm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "def get_idx_collection(items_to_add, collection, merge_collection=True):\n",
    "    \"\"\"\n",
    "    For each query (passage) in a given list, assign a qid (pid) to it, and add it to an existing query (passage)\n",
    "    collection if the query (passage) is a valid string that has never occurred in the collection\n",
    "\n",
    "    :param items_to_add: the list of queries (passages)\n",
    "    :param collection: existing query (passage) collection, which is a dictionary:\n",
    "                       {qid1: query1, qid2: query2, ...} ({pid1: passage1, pid2: passage2, ...})\n",
    "    :param merge_collection: whether to merge the new collection to the existing collection\n",
    "    :return: a list of qid (pid) of all the queries (passages) in the given list, and a collection\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    assert len(list(collection.values())) == len(set(collection.values()))\n",
    "\n",
    "    items_new = sorted(list(set(items_to_add) - set(collection.values())))\n",
    "    next_idx = max(set(collection.keys())) + 1 if len(collection) > 0 else 1\n",
    "    collection_new = dict(zip(list(range(next_idx, next_idx + len(items_new))), items_new))\n",
    "\n",
    "    reversed_collection = {v: k for k, v in collection.items()}\n",
    "    reversed_collection_new = {v: k for k, v in collection_new.items()}\n",
    "\n",
    "    if merge_collection:\n",
    "        collection_final = {**collection, **collection_new}\n",
    "        reversed_collection_final = {**reversed_collection, **reversed_collection_new}\n",
    "    else:\n",
    "        collection_final = collection_new\n",
    "        reversed_collection_final = reversed_collection_new\n",
    "    print('{0:d} unique items existed, {1:d} unique items added, {2:d} unique items in returned collection'.format(\n",
    "        len(set(items_to_add)) - len(items_new), len(items_new), len(collection_final)))\n",
    "\n",
    "    idx = list(map(reversed_collection_final.get, items_to_add))\n",
    "    # The following assertion guarantees that:\n",
    "    # when merge_collection is False, items_to_add does not contain any items in list(collection.values())\n",
    "    assert all(i is not None for i in idx)\n",
    "\n",
    "    return idx, collection_final\n",
    "\n",
    "def is_sorted(x, ascending=True):\n",
    "    if ascending:\n",
    "        return all(x[i] <= x[i + 1] for i in range(len(x) - 1))\n",
    "    else:\n",
    "        return all(x[i] >= x[i + 1] for i in range(len(x) - 1))\n",
    "    \n",
    "def visual_clustering(x, dim, visual_method, cluster_label=None, plot_noisy_points=False, plot=True):\n",
    "    x = copy.deepcopy(x)\n",
    "    \n",
    "    if cluster_label is None:\n",
    "        cluster_label = np.array([0] * x.shape[0])\n",
    "        \n",
    "    counter = Counter(cluster_label)\n",
    "    for i in range(cluster_label.shape[0]):\n",
    "        if counter[cluster_label[i]] == 1:\n",
    "            cluster_label[i] = -1\n",
    "    \n",
    "    n_noisy_points = list(cluster_label).count(-1)\n",
    "    cluster_label_unique = list(np.sort(np.unique(cluster_label)))\n",
    "    if len(cluster_label_unique) > 1:\n",
    "        print('There are {0:d} clusters, including a cluster of {1:d} noisy points'.format(len(cluster_label_unique), n_noisy_points))\n",
    "\n",
    "    assert x.shape[0] == cluster_label.shape[0]\n",
    "    print('Shape of input array: {:}'.format(x.shape))\n",
    "    \n",
    "    if visual_method == 'pca':\n",
    "        pca = PCA(n_components=dim, random_state=0)\n",
    "        x_output = pca.fit_transform(x)\n",
    "        print('Percentage of variance explained by the first {0:d} principal components: {1:}'.format(dim, pca.explained_variance_ratio_))\n",
    "        print('Shape of PCA array: {:}'.format(x_output.shape))\n",
    "    else:\n",
    "        assert visual_method == 'mds'\n",
    "        mds = MDS(dim, n_init=32, n_jobs=-1, random_state=0, dissimilarity='precomputed')\n",
    "        # cosine_dissim = cdist(x, x, 'cosine')  # This function is slow\n",
    "        cosine_dissim = 1 - x @ np.transpose(x)  # Rows of x must be normalized to 1\n",
    "        x_output = mds.fit_transform(cosine_dissim)\n",
    "        print('Shape of MDS array: {:}'.format(x_output.shape))\n",
    "    \n",
    "    if plot:\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        colors = cmap(np.linspace(0, 1.0, len(cluster_label_unique)))\n",
    "        if dim == 2:\n",
    "            plt.rcParams['figure.figsize'] = [12, 12]\n",
    "            for i, row in enumerate(x_output):\n",
    "                if cluster_label[i] == -1 and (not plot_noisy_points):\n",
    "                    continue\n",
    "                plt.scatter(row[0], row[1], c='white')\n",
    "                plt.text(row[0], row[1], cluster_label[i], c=colors[cluster_label_unique.index(cluster_label[i])], fontsize=8)\n",
    "            plt.show()\n",
    "        else:\n",
    "            assert dim == 3\n",
    "            angles = list(range(-90, 210, 30))\n",
    "            for angle in angles:\n",
    "                fig = plt.figure(figsize=(18, 18))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                for i, row in enumerate(x_output):\n",
    "                    if cluster_label[i] == -1 and (not plot_noisy_points):\n",
    "                        continue\n",
    "                    ax.scatter(row[0], row[1], row[2], c='white')\n",
    "                    ax.text(row[0], row[1], row[2], cluster_label[i], c=colors[cluster_label_unique.index(cluster_label[i])], fontsize=8)\n",
    "                ax.view_init(0, angle)\n",
    "                plt.show()\n",
    "                if angle != angles[-1]:\n",
    "                    clear_output(wait=True)\n",
    "        \n",
    "        if not plot_noisy_points:\n",
    "            print('{0:d} noisy points not plotted'.format(n_noisy_points))\n",
    "    \n",
    "    return x_output\n",
    "\n",
    "def do_clustering(df_query_common_sample, query_common_embed_sample_normlized, method, plot_noisy_points=False, agglomerative_distance_threshold=0.1):\n",
    "    np.random.seed(0)\n",
    "    df_query_common_sample = copy.deepcopy(df_query_common_sample)\n",
    "    query_common_embed_sample_normlized = copy.deepcopy(query_common_embed_sample_normlized)\n",
    "    \n",
    "    tic = time.time()\n",
    "    if method.startswith('optics'):\n",
    "        clustering_model = OPTICS(min_samples=5, max_eps=0.05, metric='cosine', cluster_method='xi', n_jobs=-1)\n",
    "    elif method.startswith('dbscan'):\n",
    "        clustering_model = DBSCAN(eps=0.05, min_samples=5, metric='cosine', n_jobs=-1)\n",
    "    elif method.startswith('agglomerative'):\n",
    "        clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='complete', distance_threshold=agglomerative_distance_threshold)\n",
    "    elif method.startswith('kmeans'):\n",
    "        clustering_model = KMeans(n_clusters=100, n_init=64, random_state=0, n_jobs=-1, algorithm='full')\n",
    "    cluster_label = clustering_model.fit_predict(query_common_embed_sample_normlized)\n",
    "    toc = time.time()\n",
    "    print('Clustering takes {:.2f} second(s)'.format(toc - tic))\n",
    "    \n",
    "    output_pca2 = visual_clustering(query_common_embed_sample_normlized, dim=2, visual_method='pca', cluster_label=cluster_label, plot_noisy_points=plot_noisy_points)\n",
    "    if query_common_embed_sample_normlized.shape[0] < 12000:\n",
    "        output_mds2 = visual_clustering(query_common_embed_sample_normlized, dim=2, visual_method='mds', cluster_label=cluster_label, plot_noisy_points=plot_noisy_points)\n",
    "        \n",
    "    cluster_col_name = 'cluster_' + method\n",
    "    df_query_common_sample[cluster_col_name] = list(cluster_label)\n",
    "    \n",
    "    pd.set_option('display.max_rows', min(1000, df_query_common_sample.shape[0] + 1))\n",
    "    return df_query_common_sample, cluster_col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_original = pd.read_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/help_search_queries_d2_benchmark.tsv', sep='\\t', index_col=False).astype({'qid': str, 'query': str})\n",
    "df_query_original = df_query_original.rename(columns={'qid': 'session'})\n",
    "df_query_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_counts = df_query_original['query'].value_counts()\n",
    "dict_value_counts = dict(zip(list(df_value_counts.index), df_value_counts.to_list()))\n",
    "\n",
    "df_query = df_query_original.drop_duplicates('query').reset_index(drop=True)\n",
    "df_query['freq'] = df_query['query'].map(dict_value_counts.get)\n",
    "df_query = df_query.sort_values('freq', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_query['qid'], _ = get_idx_collection(df_query['query'], dict())\n",
    "assert df_query['qid'].is_unique and (not df_query['qid'].isnull().any()) and (not df_query['query'].isnull().any())\n",
    "df_query = df_query.loc[:, ['session', 'qid', 'query', 'freq']]\n",
    "\n",
    "df_query.to_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/help_search_queries_d2_benchmark_unique.tsv', sep='\\t', index=False)\n",
    "df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = pd.read_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/help_search_queries_d2_benchmark_unique.tsv', sep='\\t', index_col=False).astype({'session': str, 'qid': int, 'query': str, 'freq': int})\n",
    "\n",
    "df_query_common = df_query.loc[df_query['freq'] > 1, :].reset_index(drop=True)\n",
    "\n",
    "df_query_common.to_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/help_search_queries_d2_benchmark_unique_common.tsv', sep='\\t', index=False)\n",
    "df_query_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply QSM to get embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_common = pd.read_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/help_search_queries_d2_benchmark_unique_common.tsv', sep='\\t', index_col=False).astype({'session': str, 'qid': int, 'query': str, 'freq': int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = '/home/yqinamz/output/EXP_1002/qa380_Mix_1USE_9R-sbert-2020-10-09_19-14-39/'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "gpu = '7'\n",
    "device = torch.device('cuda:{:}'.format(gpu) if torch.cuda.is_available() and gpu != '' else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_common_embed = model.encode(df_query_common['query'].to_list())\n",
    "np.save('/data/qyouran/QABot/d2_benchmark/query_common_embed.npy', query_common_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_common_embed = np.load('/data/qyouran/QABot/d2_benchmark/query_common_embed.npy')\n",
    "\n",
    "query_common_embed_normlized = query_common_embed / np.linalg.norm(query_common_embed, axis=1, keepdims=True)\n",
    "assert np.isclose(np.diagonal(query_common_embed_normlized[:1000, :] @ np.transpose(query_common_embed_normlized[:1000, :])), np.diagonal(np.identity(1000))).all()\n",
    "\n",
    "# Impossible to create a very big dissimilarity matrix, so we have to randomly sample some rows\n",
    "np.random.seed(0)\n",
    "# sampled_index = np.sort(np.random.choice(query_common_embed_normlized.shape[0], size=1000, replace=False))  # len(sampled_index) randomly sampled queries\n",
    "sampled_index = np.arange(500)  # The first len(sampled_index) most frequent queries\n",
    "\n",
    "query_common_embed_normlized_sample = query_common_embed_normlized[sampled_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_common_embed = pd.DataFrame(query_common_embed)\n",
    "df_query_common_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_common_embed.min())\n",
    "print(query_common_embed.max())\n",
    "describe = df_query_common_embed.describe()\n",
    "describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_common_embed_normlized_sample_mds2 = visual_clustering(query_common_embed_normlized_sample, dim=2, visual_method='mds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_common_embed_normlized_sample_mds3 = visual_clustering(query_common_embed_normlized_sample, dim=3, visual_method='mds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_common_embed_normlized_sample_pca2 = visual_clustering(query_common_embed_normlized_sample, dim=2, visual_method='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_common_embed_normlized_sample_pca3 = visual_clustering(query_common_embed_normlized_sample, dim=3, visual_method='pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_common = pd.read_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/help_search_queries_d2_benchmark_unique_common.tsv', sep='\\t', index_col=False)\n",
    "query_common_embed = np.load('/data/qyouran/QABot/d2_benchmark/query_common_embed.npy')\n",
    "assert df_query_common.shape[0] == query_common_embed.shape[0] and is_sorted(df_query_common['freq'].to_list(), ascending=False) and list(df_query_common.index) == list(range(df_query_common.shape[0])) \n",
    "\n",
    "n = min(list(df_query_common.loc[df_query_common['freq'] >= 50, :].index)[-1] + 1, 2000)\n",
    "df_query_common_sample = copy.deepcopy(df_query_common.iloc[:n, :])\n",
    "query_common_embed_sample = copy.deepcopy(query_common_embed[:n, :])\n",
    "\n",
    "query_common_embed_sample_normlized = query_common_embed_sample / np.linalg.norm(query_common_embed_sample, axis=1, keepdims=True)\n",
    "assert np.isclose(np.diagonal(query_common_embed_sample_normlized[:min(n, 1000), :] @ np.transpose(query_common_embed_sample_normlized[:min(n, 1000), :])), np.diagonal(np.identity(min(n, 1000)))).all()\n",
    "\n",
    "print('The queries we focus on are {0:d} / {1:d} = {2:.2f}% of all the help search queries'.format(df_query_common_sample['freq'].sum(), 20889412, 100 * df_query_common_sample['freq'].sum() / 20889412))\n",
    "df_query_common_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_query_common_sample_clustered, cluster_col_name = do_clustering(df_query_common_sample, query_common_embed_sample_normlized, method='agglomerative_0.06', agglomerative_distance_threshold=0.06)\n",
    "print(df_query_common_sample_clustered[cluster_col_name].value_counts())\n",
    "\n",
    "assert df_query_common_sample_clustered.drop(columns=cluster_col_name).equals(df_query_common.iloc[:n, :])\n",
    "df_query_common_sample_clustered.to_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/point06agglomerative_top2000/help_search_queries_d2_benchmark_unique_common_clustered_point06agglomerative_top2000.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select queries for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1200)\n",
    "\n",
    "df_clustered = pd.read_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/point06agglomerative_top2000/help_search_queries_d2_benchmark_unique_common_clustered_point06agglomerative_top2000.tsv', sep='\\t', index_col=False)\n",
    "cluster_col_name = 'cluster_agglomerative_0.06'\n",
    "assert is_sorted(df_clustered['freq'].to_list(), ascending=False) and list(df_clustered.index) == list(range(df_clustered.shape[0]))\n",
    "\n",
    "# n = 600 # Among the 980 unique most high-frequent queries, 600 unique queries are selected for annotation and 380 unique queries that are similar to them are removed. Although we actually annoate 600 unique queries, we \"effectively\" annotate 980 unique queries, which are 6833673 / 20889412 = 32.71% of all help search queries\n",
    "n = 1100  # Among the 1973 unique most high-frequent queries, 1100 unique queries are selected for annotation and 873 unique queries that are similar to them are removed. Although we actually annoate 1100 unique queries, we \"effectively\" annotate 1973 unique queries, which are 7951698 / 20889412 = 38.07% of all help search queries\n",
    "\n",
    "index_selected = list()\n",
    "index_removed = list()\n",
    "list_query_already_selected = list()\n",
    "dict_query_already_selected = dict()\n",
    "\n",
    "for i in df_clustered.index:\n",
    "    cluster_label = df_clustered.loc[i, cluster_col_name]\n",
    "    query = df_clustered.loc[i, 'query']\n",
    "    if cluster_label not in dict_query_already_selected:\n",
    "        if cluster_label != -1:\n",
    "            dict_query_already_selected[cluster_label] = query\n",
    "        index_selected.append(i)\n",
    "    else:\n",
    "        assert cluster_label != -1\n",
    "        list_query_already_selected.append(dict_query_already_selected[cluster_label])\n",
    "        index_removed.append(i)\n",
    "    if len(index_selected) == n:\n",
    "        break\n",
    "assert len(index_selected) == n and len(index_removed) == i + 1 - n\n",
    "\n",
    "df_selected = copy.deepcopy(df_clustered.loc[index_selected, :])\n",
    "df_removed = copy.deepcopy(df_clustered.loc[index_removed, :])\n",
    "df_removed['query_selected_for_this_cluster'] = list_query_already_selected\n",
    "\n",
    "n_nonunique_effective = df_selected['freq'].sum() + df_removed['freq'].sum()\n",
    "print('Among the {0:d} unique most high-frequent queries, {1:d} unique queries are selected for annotation and {2:d} unique queries that are similar to them are removed. Although we actually annoate {1:d} unique queries, we \"effectively\" annotate {0:d} unique queries, which are {3:d} / {4:d} = {5:.2f}% of all help search queries'.format(\n",
    "      i + 1, n, i + 1 - n, n_nonunique_effective, 20889412, 100 * n_nonunique_effective / 20889412))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert is_sorted(df_selected['freq'].to_list(), ascending=False)\n",
    "df_selected.iloc[:600, :].to_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/point06agglomerative_top2000/help_search_queries_d2_benchmark_selected_1-600.tsv', sep='\\t', index=False)\n",
    "df_selected.iloc[600:, :].to_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/point06agglomerative_top2000/help_search_queries_d2_benchmark_selected_601-1100.tsv', sep='\\t', index=False)\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert is_sorted(df_removed['freq'].to_list(), ascending=False)\n",
    "df_removed.to_csv('/data/qyouran/QABot/d2_benchmark/help_search_queries/point06agglomerative_top2000/help_search_queries_d2_benchmark_removed.tsv', sep='\\t', index=False)\n",
    "df_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "query_common_embed = np.load('/data/qyouran/QABot/d2_benchmark/query_common_embed.npy')\n",
    "for i in df_removed.index:\n",
    "    selected_query = df_removed.loc[i, 'query_selected_for_this_cluster']\n",
    "    index = df_clustered.loc[df_clustered['query'] == selected_query, :].index\n",
    "    assert len(index) == 1 and (1 - cdist(query_common_embed[i].reshape(1, -1), query_common_embed[index[0]].reshape(1, -1), 'cosine'))[0, 0] >= 0.94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qabot",
   "language": "python",
   "name": "qabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
