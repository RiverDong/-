{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/natural_questions\n",
    "# https://github.com/huggingface/datasets\n",
    "# https://github.com/huggingface/datasets/issues/2401\n",
    "\n",
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric  # The version of \"datasets\" I used is 1.5.0\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', 5000)\n",
    "\n",
    "\n",
    "def insert_to_list(x, inserted):\n",
    "    # inserted is a list of tuples: [(index1, value1), (index2, value2), ...], \n",
    "    # which means we will simultaneously insert value1 immediately before x[index1], value2 immediately before x[index2], ...\n",
    "    x = copy.deepcopy(x)\n",
    "    inserted = sorted(inserted, key=lambda x: x[0])\n",
    "    for i, (index, value) in enumerate(inserted):\n",
    "        x.insert(index + i, value)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_nq_samples(x, split_type='train', remove_html_tokens=True):\n",
    "    # In most cases, x should be a single NQ datapoint, e.g., nq_dataset['train'][0] or nq_dataset['validation'][0]\n",
    "    \n",
    "    # However, to speed up \"get_all_nq_samples_parallel\", we allow x to be the index of an NQ datapoint,\n",
    "    # and in this case the function will fetch the NQ datapoint from the global variable \"nq_dataset\" by \"x = nq_dataset[split_type][x]\"\n",
    "    # Note that the argument \"split_type\" is only used at here\n",
    "    if isinstance(x, int):\n",
    "        x = nq_dataset[split_type][x]\n",
    "\n",
    "    samples = []\n",
    "    \n",
    "    sample_idx = x['id']\n",
    "    \n",
    "    question = x['question']['text']\n",
    "    \n",
    "    document_url = x['document']['url']\n",
    "    \n",
    "    document_title = x['document']['title']\n",
    "    \n",
    "    assert len(x['document']['tokens']['token']) == len(x['document']['tokens']['is_html']), x\n",
    "    token_is_html = list(zip(x['document']['tokens']['token'], x['document']['tokens']['is_html']))\n",
    "    \n",
    "    document_list = [token for token, is_html in token_is_html if (not remove_html_tokens) or (not is_html)]\n",
    "    document = ' '.join(document_list)\n",
    "    \n",
    "    assert len(x['annotations']['id']) == len(x['annotations']['long_answer']), x\n",
    "    assert len(x['annotations']['id']) == len(x['annotations']['short_answers']), x\n",
    "    assert len(x['annotations']['id']) == len(x['annotations']['yes_no_answer']), x\n",
    "    for i in range(len(x['annotations']['id'])):\n",
    "        annotation_idx = x['annotations']['id'][i]\n",
    "                \n",
    "        s_start_token, s_end_token = x['annotations']['short_answers'][i]['start_token'], x['annotations']['short_answers'][i]['end_token']\n",
    "        if len(s_start_token) >= 1 and len(s_end_token) >= 1:\n",
    "            assert len(s_start_token) == len(s_end_token), x\n",
    "            \n",
    "            short_answers_list = []\n",
    "            for j in range(len(s_start_token)):\n",
    "                assert s_start_token[j] != -1 and s_end_token[j] != -1, x\n",
    "                short_answers_list += [token for token, is_html in token_is_html[s_start_token[j]:s_end_token[j]] if (not remove_html_tokens) or (not is_html)]\n",
    "            \n",
    "            short_answers = ' '.join(short_answers_list)\n",
    "            short_answers_type = 'MULTIPLE_SPANS' if len(s_start_token) > 1 else 'SINGLE_SPAN'\n",
    "        else:\n",
    "            short_answers = ''\n",
    "            short_answers_type = 'EMPTY'\n",
    "            \n",
    "        l_start_token, l_end_token = x['annotations']['long_answer'][i]['start_token'], x['annotations']['long_answer'][i]['end_token']\n",
    "        if l_start_token != -1 and l_end_token != -1:\n",
    "            long_answer_list = [token for token, is_html in token_is_html[l_start_token:l_end_token] if (not remove_html_tokens) or (not is_html)]\n",
    "            long_answer = ' '.join(long_answer_list)\n",
    "        else:\n",
    "            long_answer = ''\n",
    "            \n",
    "        if l_start_token != -1 and l_end_token != -1:\n",
    "            inserted = []\n",
    "            for s_start_token_j, s_end_token_j in zip(s_start_token, s_end_token):\n",
    "                assert s_start_token_j != -1 and s_end_token_j != -1, x\n",
    "                inserted.append((s_start_token_j, ('<hl>', False)))\n",
    "                inserted.append((s_end_token_j, ('<hl>', False)))\n",
    "            inserted.append((l_start_token, ('<START_OF_LONG_ANSWER>', False)))\n",
    "            inserted.append((l_end_token, ('<END_OF_LONG_ANSWER>', False)))\n",
    "            token_is_html_highlighted = insert_to_list(token_is_html, inserted)\n",
    "            \n",
    "            hl_start_token = token_is_html_highlighted.index(('<START_OF_LONG_ANSWER>', False))\n",
    "            hl_end_token = token_is_html_highlighted.index(('<END_OF_LONG_ANSWER>', False))\n",
    "            \n",
    "            long_answer_highlighted_list = [token for token, is_html in token_is_html_highlighted[(hl_start_token + 1):hl_end_token] if (not remove_html_tokens) or (not is_html)]\n",
    "            assert long_answer_highlighted_list.count('<hl>') == len(inserted) - 2, x\n",
    "            assert len(long_answer_highlighted_list) - len(long_answer_list) == len(inserted) - 2, x\n",
    "            assert (set(long_answer_highlighted_list) - set(long_answer_list)).issubset({'<hl>'}), x\n",
    "            assert '<START_OF_LONG_ANSWER>' not in long_answer_highlighted_list and '<END_OF_LONG_ANSWER>' not in long_answer_highlighted_list, x\n",
    "            long_answer_highlighted = ' '.join(long_answer_highlighted_list)\n",
    "        else:\n",
    "            long_answer_highlighted = ''\n",
    "        \n",
    "        if x['annotations']['yes_no_answer'][i] != -1:\n",
    "            yes_no_answer = 'YES' if bool(x['annotations']['yes_no_answer'][i]) else 'NO'\n",
    "        else:\n",
    "            yes_no_answer = ''\n",
    "            \n",
    "        if len(s_start_token) >= 1 and len(s_end_token) >= 1:\n",
    "            inserted = []\n",
    "            for s_start_token_j, s_end_token_j in zip(s_start_token, s_end_token):\n",
    "                assert s_start_token_j != -1 and s_end_token_j != -1, x\n",
    "                inserted.append((s_start_token_j, ('<hl>', False)))\n",
    "                inserted.append((s_end_token_j, ('<hl>', False)))\n",
    "            token_is_html_highlighted = insert_to_list(token_is_html, inserted)\n",
    "\n",
    "            document_highlighted_list = [token for token, is_html in token_is_html_highlighted if (not remove_html_tokens) or (not is_html)]\n",
    "            assert document_highlighted_list.count('<hl>') == len(inserted), x\n",
    "            assert len(document_highlighted_list) - len(document_list) == len(inserted), x\n",
    "            assert (set(document_highlighted_list) - set(document_list)) == {'<hl>'}, x\n",
    "            document_highlighted = ' '.join(document_highlighted_list)\n",
    "        else:\n",
    "            document_highlighted = document\n",
    "            \n",
    "        assert long_answer in document, x\n",
    "        assert (short_answers_type == 'MULTIPLE_SPANS') or (short_answers in long_answer), x\n",
    "        samples.append({'sample_idx': sample_idx, 'question': question, 'document_url': document_url, 'document_title': document_title, \n",
    "                        'document': document, 'document_highlighted': document_highlighted, 'annotation_idx': annotation_idx, 'long_answer': long_answer, 'long_answer_highlighted': long_answer_highlighted, \n",
    "                        'short_answers': short_answers, 'short_answers_type': short_answers_type, 'yes_no_answer': yes_no_answer})\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_all_nq_samples(dataset, split_type, remove_html_tokens=True):\n",
    "    dataset_split = dataset[split_type]\n",
    "    all_samples = []\n",
    "    for i in tqdm(range(len(dataset_split))):\n",
    "        all_samples.extend(get_nq_samples(dataset_split[i], remove_html_tokens=remove_html_tokens))\n",
    "    return pd.DataFrame(all_samples)\n",
    "\n",
    "\n",
    "def get_all_nq_samples_parallel(dataset, split_type, remove_html_tokens=True, n_cpu=psutil.cpu_count(logical=True)):\n",
    "    with Pool(n_cpu) as pool:\n",
    "        all_samples = pool.starmap(get_nq_samples, [(i, split_type, remove_html_tokens) for i in range(len(dataset[split_type]))])\n",
    "    return pd.DataFrame(list(itertools.chain(*all_samples)))\n",
    "\n",
    "\n",
    "def format_nq_train_val(df, train_or_val, use_long_answer_as_passage=False):\n",
    "    df = df.fillna('')\n",
    "    df_selected = copy.deepcopy(df.loc[df['short_answers_type'] == 'SINGLE_SPAN', :].reset_index(drop=True))\n",
    "    \n",
    "    for col in df_selected.columns:\n",
    "        if col == 'yes_no_answer':\n",
    "            assert (df_selected[col] == '').all()\n",
    "        else:\n",
    "            assert (df_selected[col] != '').all()\n",
    "    for i in df_selected.index:\n",
    "        assert df_selected.loc[i, 'long_answer'] in df_selected.loc[i, 'document']\n",
    "        assert df_selected.loc[i, 'short_answers'] in df_selected.loc[i, 'long_answer']\n",
    "        \n",
    "        long_answer_highlighted_list = df_selected.loc[i, 'long_answer_highlighted'].split(' ')\n",
    "        hl_indices = [idx for idx, token in enumerate(long_answer_highlighted_list) if token == '<hl>']\n",
    "        assert len(hl_indices) == 2\n",
    "        assert ' '.join(long_answer_highlighted_list[(hl_indices[0] + 1):hl_indices[1]]) == df_selected.loc[i, 'short_answers']\n",
    "        \n",
    "        document_highlighted_list = df_selected.loc[i, 'document_highlighted'].split(' ')\n",
    "        hl_indices = [idx for idx, token in enumerate(document_highlighted_list) if token == '<hl>']\n",
    "        assert len(hl_indices) == 2\n",
    "        assert ' '.join(document_highlighted_list[(hl_indices[0] + 1):hl_indices[1]]) == df_selected.loc[i, 'short_answers']\n",
    "    \n",
    "    if train_or_val == 'val':\n",
    "        # Each question in the validation set has 5 annotations\n",
    "        # We keep only one annotation whose short answer is the most frequent\n",
    "        index_selected = []\n",
    "        for question in df_selected['question'].unique():\n",
    "            sub_df = df_selected.loc[df_selected['question'] == question, :]\n",
    "            answer_value_counts = copy.deepcopy(sub_df['short_answers'].value_counts().to_frame(name='short_answers_count'))\n",
    "            answer_value_counts['short_answers_sort'] = answer_value_counts.index\n",
    "            most_freq_answer = answer_value_counts.sort_values(['short_answers_count', 'short_answers_sort'], ascending=False)['short_answers_sort'].iloc[0]\n",
    "            index_selected.append(sub_df.index[sub_df['short_answers'] == most_freq_answer][0])\n",
    "        df_selected = df_selected.loc[index_selected, :].reset_index(drop=True)\n",
    "    assert not df_selected['question'].duplicated().any()\n",
    "    \n",
    "    if use_long_answer_as_passage:\n",
    "        df_selected = df_selected.loc[:, ['question', 'short_answers', 'long_answer', 'long_answer_highlighted']].rename(columns={'question': 'query', 'short_answers': 'answer', 'long_answer': 'passage', 'long_answer_highlighted': 'passage_hl'}).reset_index(drop=True)\n",
    "    else:\n",
    "        df_selected = df_selected.loc[:, ['question', 'short_answers', 'document', 'document_highlighted']].rename(columns={'question': 'query', 'short_answers': 'answer', 'document': 'passage', 'document_highlighted': 'passage_hl'}).reset_index(drop=True)\n",
    "    \n",
    "    return df_selected\n",
    "\n",
    "\n",
    "def get_idx_collection(items_to_add, collection, merge_collection=True):\n",
    "    \"\"\"\n",
    "    For each query (passage) in a given list, assign a qid (pid) to it, and add it to an existing query (passage)\n",
    "    collection if the query (passage) is a valid string that has never occurred in the collection\n",
    "\n",
    "    :param items_to_add: the list of queries (passages)\n",
    "    :param collection: existing query (passage) collection, which is a dictionary:\n",
    "                       {qid1: query1, qid2: query2, ...} ({pid1: passage1, pid2: passage2, ...})\n",
    "    :param merge_collection: whether to merge the new collection to the existing collection\n",
    "    :return: a list of qid (pid) of all the queries (passages) in the given list, and a collection\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    assert len(list(collection.values())) == len(set(collection.values()))\n",
    "\n",
    "    items_new = sorted(list(set(items_to_add) - set(collection.values())))\n",
    "    next_idx = max(set(collection.keys())) + 1 if len(collection) > 0 else 1\n",
    "    collection_new = dict(zip(list(range(next_idx, next_idx + len(items_new))), items_new))\n",
    "\n",
    "    reversed_collection = {v: k for k, v in collection.items()}\n",
    "    reversed_collection_new = {v: k for k, v in collection_new.items()}\n",
    "\n",
    "    if merge_collection:\n",
    "        collection_final = {**collection, **collection_new}\n",
    "        reversed_collection_final = {**reversed_collection, **reversed_collection_new}\n",
    "    else:\n",
    "        collection_final = collection_new\n",
    "        reversed_collection_final = reversed_collection_new\n",
    "    print('{0:d} unique items existed, {1:d} unique items added, {2:d} unique items in returned collection'.format(\n",
    "        len(set(items_to_add)) - len(items_new), len(items_new), len(collection_final)))\n",
    "\n",
    "    idx = list(map(reversed_collection_final.get, items_to_add))\n",
    "    # The following assertion guarantees that:\n",
    "    # when merge_collection is False, items_to_add does not contain any items in list(collection.values())\n",
    "    assert all(i is not None for i in idx)\n",
    "\n",
    "    return idx, collection_final\n",
    "\n",
    "\n",
    "def is_1_to_1(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    d = dict()\n",
    "    for i in tqdm(range(len(x))):\n",
    "        if x[i] not in d:\n",
    "            if y[i] in set(d.values()):\n",
    "                return False\n",
    "            d[x[i]] = y[i]\n",
    "        else:\n",
    "            if d[x[i]] != y[i]:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the available datasets\n",
    "# print(list_datasets())\n",
    "\n",
    "# Load a dataset and print the first example in the training set\n",
    "nq_dataset = load_dataset('natural_questions', cache_dir='/data/qyouran3/huggingface_datasets/natural_questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nq_train_raw = get_all_nq_samples_parallel(nq_dataset, 'train')\n",
    "nq_train_raw.to_csv('/data/qyouran3/QABot/natural-questions-data/nq_train_raw.tsv', sep='\\t', index=False)\n",
    "nq_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nq_val_raw = get_all_nq_samples_parallel(nq_dataset, 'validation')\n",
    "nq_val_raw.to_csv('/data/qyouran3/QABot/natural-questions-data/nq_val_raw.tsv', sep='\\t', index=False)\n",
    "nq_val_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train_raw = pd.read_csv('/data/qyouran3/QABot/natural-questions-data/nq_train_raw.tsv', sep='\\t', index_col=False).fillna('')\n",
    "# nq_train_raw.loc[nq_train_raw['sample_idx'] == 947884478026891751, 'document_title'] = 'NaN'\n",
    "\n",
    "nq_train = format_nq_train_val(nq_train_raw, train_or_val='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nq_val_raw = pd.read_csv('/data/qyouran3/QABot/natural-questions-data/nq_val_raw.tsv', sep='\\t', index_col=False).fillna('')\n",
    "\n",
    "nq_val = format_nq_train_val(nq_val_raw, train_or_val='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train_val = nq_train.append(nq_val).reset_index(drop=True)\n",
    "\n",
    "qid_col, _ = get_idx_collection(nq_train_val['query'].to_list(), dict())\n",
    "nq_train_val.insert(0, 'qid', qid_col)\n",
    "\n",
    "pid_col, _ = get_idx_collection(nq_train_val['passage'].to_list(), dict())\n",
    "pid_col = ['P' + str(pid) for pid in pid_col]\n",
    "nq_train_val.insert(nq_train_val.shape[1], 'pid', pid_col)\n",
    "\n",
    "assert is_1_to_1(nq_train_val['qid'].to_list(), nq_train_val['query'].to_list())\n",
    "assert is_1_to_1(nq_train_val['pid'].to_list(), nq_train_val['passage'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nq_train_final = nq_train_val.iloc[:nq_train.shape[0], :].reset_index(drop=True)\n",
    "nq_train_final.to_csv('/data/qyouran3/QABot/natural-questions-data/nq_train.tsv', sep='\\t', index=False)\n",
    "nq_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nq_val_final = nq_train_val.iloc[nq_train.shape[0]:, :].reset_index(drop=True)\n",
    "nq_val_final.to_csv('/data/qyouran3/QABot/natural-questions-data/nq_val.tsv', sep='\\t', index=False)\n",
    "nq_val_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qabot",
   "language": "python",
   "name": "qabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
